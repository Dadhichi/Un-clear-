{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment3_CGAN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"Z0NjvDSYYup4"},"source":["%tensorflow_version 2.x "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7hTyijbLB0B"},"source":["import tensorflow as tf\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import os\n","import time\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBgNGbqnMe3a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TC3CEOpNO5pL"},"source":["%cd /content/drive/MyDrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bSr4x6LENS_W"},"source":["os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/\"\n","\n","!kaggle datasets download -d ktaebum/anime-sketch-colorization-pair\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_bS3ze0dPlbA"},"source":["!unzip \\*.zip && rm *.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CeptAdNMFNYf"},"source":["%ls\n","%pwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d5kWa89TTVqk"},"source":["imgs_path = 'data/data/train/'\n","images = os.listdir(imgs_path)\n","print(len(images))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNFNNfwqlFEP"},"source":["test_imgs_path = 'data/data/val/'\n","test_images = os.listdir(test_imgs_path)\n","print(len(test_images))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7S_tWASWoAwE"},"source":["new_images = []\n","for i in images:\n","  i = 'data/train/'+i\n","  new_images.append(i)\n","new_test_images = []\n","for i in test_images:\n","  i = 'data/val/'+i\n","  new_test_images.append(i)\n","images = new_images\n","test_images = new_test_images"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Exv--Qw-YKqU"},"source":["%pwd\n","%cd data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oloOQyQSb7Kw"},"source":["#%cd train\n","\n","img = tf.keras.preprocessing.image.load_img(random.choice(images))\n","img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9P2r5DuelvdA"},"source":["test_img = tf.keras.preprocessing.image.load_img(random.choice(test_images))\n","test_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mthUUAtfck5A"},"source":["def read_jpg(path):\n","    img = tf.io.read_file(path)\n","    img = tf.image.decode_jpeg(img,channels=3)\n","    return img \n","\n","def normalise(image,mask):\n","    image = tf.cast(image,tf.float32)/127.5 - 1\n","    mask = tf.cast(mask,tf.float32)/127.5 - 1\n","    return image , mask\n","\n","def load_image(path):\n","    input_image = read_jpg(path)\n","    w = tf.shape(input_image)[1]\n","    w = w//2\n","    image = input_image[:,:w,:]\n","    mask = input_image[:,w:,:]\n","    image = tf.image.resize(image, (256, 256))\n","    mask = tf.image.resize(mask, (256, 256))\n","    if tf.random.uniform(()) > 0.5 :\n","        image = tf.image.flip_left_right(image)\n","        mask = tf.image.flip_left_right(image)\n","    image , mask = normalise(image,mask)\n","    return mask,image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YD3HIXgjXBxR"},"source":["def show(image,mask):\n","    plt.subplot(1,2,1)\n","    plt.imshow(image)\n","    plt.subplot(1,2,2)\n","    plt.imshow(mask)\n","    return None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wt8EqpfbfWSH"},"source":["%pwd\n","image,mask = load_image(images[0])\n","print(tf.shape(image)[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfZeEmpDgqVn"},"source":["print(images[0])\n","l = []\n","for img in images:\n","    rand = imgs_path+str(img)\n","    l.append(rand)\n","print(imgs_path)\n","print(l[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1VQwJ26eqR0"},"source":["dataset = tf.data.Dataset.from_tensor_slices(images)\n","train = dataset.map(load_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n","train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATccTtrfmTqv"},"source":["test_dataset = tf.data.Dataset.from_tensor_slices(test_images)\n","test = dataset.map(load_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n","test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S9-gnMG3UGD5"},"source":["BATCH_SIZE = 4\n","BUFFER_SIZE = 4000 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lFdDIV7dUcYj"},"source":["train_dataset = train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","print(\":)\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXMfmoXxnHxR"},"source":["test_dataset = test.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6TcPuhOTgrZh"},"source":["for i in train_dataset.take(1):\n","  print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nYXSFgSmkVPA"},"source":["OUTPUT_CHANNELS = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QC22S2_bjB8k"},"source":["def downsample(filters, size, apply_batchnorm=True):\n","    result = tf.keras.Sequential()\n","    result.add(\n","        tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n","                               use_bias=False))\n","    if apply_batchnorm:\n","        result.add(tf.keras.layers.BatchNormalization())\n","        result.add(tf.keras.layers.LeakyReLU())\n","\n","    return result\n","\n","def upsample(filters, size, apply_dropout=False):\n","    result = tf.keras.Sequential()\n","    result.add(\n","        tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n","                                        use_bias=False))\n","    result.add(tf.keras.layers.BatchNormalization())\n","    if apply_dropout:\n","        result.add(tf.keras.layers.Dropout(0.5))\n","    result.add(tf.keras.layers.ReLU())\n","\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-09-10T17:09:56.692930Z","iopub.status.busy":"2020-09-10T17:09:56.691992Z","iopub.status.idle":"2020-09-10T17:09:56.694671Z","shell.execute_reply":"2020-09-10T17:09:56.694208Z"},"papermill":{"duration":0.061693,"end_time":"2020-09-10T17:09:56.694794","exception":false,"start_time":"2020-09-10T17:09:56.633101","status":"completed"},"tags":[],"id":"7WyZ3Ed9Hw76"},"source":["def Generator():\n","    inputs = tf.keras.layers.Input(shape=[256,256,3])\n","\n","    down_stack = [\n","        downsample(64, 4, apply_batchnorm=False), \n","        downsample(128, 4),\n","        downsample(256, 4), \n","        downsample(512, 4), \n","        downsample(512, 4), \n","        downsample(512, 4), \n","        downsample(512, 4), \n","        downsample(512, 4), \n","    ]\n","\n","    up_stack = [\n","        upsample(512, 4, apply_dropout=True), \n","        upsample(512, 4, apply_dropout=True),\n","        upsample(512, 4, apply_dropout=True), \n","        upsample(512, 4),\n","        upsample(256, 4), \n","        upsample(128, 4), \n","        upsample(64, 4),\n","    ]\n","\n","    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n","                                         strides=2,\n","                                         padding='same',\n","                                         activation='tanh')\n","    x = inputs\n","    skips = []\n","    for down in down_stack:\n","        x = down(x)\n","        skips.append(x)\n","\n","    skips = reversed(skips[:-1])\n","    for up, skip in zip(up_stack, skips):\n","        x = up(x)\n","        x = tf.keras.layers.Concatenate()([x, skip])\n","\n","    x = last(x)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3we61GbdjxB2"},"source":["generator = Generator()\n","l = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4TGaRpJfj46c"},"source":["def generator_loss(disc_generated_output, gen_output, target):\n","    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n","\n","    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","\n","    total_gen_loss = gan_loss + (l * l1_loss)\n","\n","    return total_gen_loss, gan_loss, l1_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltY_wThkkAx0"},"source":["def Discriminator():\n","    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n","    tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n","\n","    x = tf.keras.layers.concatenate([inp, tar]) \n","\n","    down1 = downsample(64, 4, False)(x) \n","    down2 = downsample(128, 4)(down1) \n","    down3 = downsample(256, 4)(down2) \n","\n","    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n","                                  padding='same',\n","                                  use_bias=False)(down3)\n","\n","    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n","\n","    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n","\n","    last = tf.keras.layers.Conv2D(1, 4, strides=1, padding='same')(leaky_relu) \n","\n","    return tf.keras.Model(inputs=[inp, tar], outputs=last)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6EJm1aekJf5"},"source":["discriminator = Discriminator()\n","loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpOUT7mfkh3N"},"source":["def discriminator_loss(disc_real_output, disc_generated_output):\n","    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n","\n","    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n","\n","    total_disc_loss = real_loss + generated_loss\n","\n","    return total_disc_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-5aHFHlkttP"},"source":["generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMr3C9nPkwH3"},"source":["def generate_images(model, test_input, tar):\n","    prediction = model(test_input, training=True)\n","    plt.figure(figsize=(15,15))\n","\n","    display_list = [test_input[0], tar[0], prediction[0]]\n","    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n","\n","    for i in range(3):\n","        plt.subplot(1, 3, i+1)\n","        plt.title(title[i])\n","    # getting the pixel values between [0, 1] to plot it.\n","        plt.imshow(display_list[i] * 0.5 + 0.5)\n","        plt.axis('off')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfTccxhaljoM"},"source":["%pwd\n","%cd MyDrive/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"efyIPWpJmPWz"},"source":["%cd data/data/val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-v_tiB-mjb2"},"source":["EPOCHS = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_UhA64mmnl1"},"source":["@tf.function\n","def train_step(input_image, target, epoch):\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        gen_output = generator(input_image, training=True)\n","\n","        disc_real_output = discriminator([input_image, target], training=True)\n","        disc_generated_output = discriminator([input_image, gen_output], training=True)\n","\n","        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n","        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n","\n","    generator_gradients = gen_tape.gradient(gen_total_loss,\n","                                          generator.trainable_variables)\n","    discriminator_gradients = disc_tape.gradient(disc_loss,\n","                                               discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(generator_gradients,\n","                                          generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n","                                              discriminator.trainable_variables))\n","    return gen_total_loss, disc_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dPBygVJamqSA"},"source":["epoch_loss_avg_gen = tf.keras.metrics.Mean('g_loss')\n","epoch_loss_avg_disc = tf.keras.metrics.Mean('d_loss')\n","g_loss_results = []\n","d_loss_results = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XA6nuA3mwyA"},"source":["def fit(train_ds, epochs, test_ds):\n","    for epoch in range(epochs+1):\n","        if epoch%10 == 0:\n","            for example_input, example_target in test_ds.take(1):\n","                generate_images(generator, example_input, example_target)\n","        print(\"Epoch: \", epoch)\n","\n","        for n, (input_image, target) in train_ds.enumerate():\n","            print('.', end='')\n","            g_loss, d_loss = train_step(input_image, target, epoch)\n","            epoch_loss_avg_gen(g_loss)\n","            epoch_loss_avg_disc(d_loss)\n","        print()\n","        g_loss_results.append(epoch_loss_avg_gen.result())\n","        d_loss_results.append(epoch_loss_avg_disc.result())\n","        \n","        epoch_loss_avg_gen.reset_states()\n","        epoch_loss_avg_disc.reset_states()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ill2CEyS2Fm"},"source":["for input, target in test_dataset.take(1):\n","    generate_images(generator, input, target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mi_GuglImz_2"},"source":["fit(train_dataset, EPOCHS, test_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPkkJNMVoNUF"},"source":[""],"execution_count":null,"outputs":[]}]}